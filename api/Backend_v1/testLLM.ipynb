{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chommakorn\n",
      "WARNING:tensorflow:From c:\\Users\\chomm\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\chomm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chomm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chomm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from PromptOps.test import PromptCompletion, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basic prompts for testing\n",
    "test_prompt = \"What is the capital of France?\"\n",
    "expected_answer = \"Paris\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configurations for each model provider\n",
    "configs = [\n",
    "    {\n",
    "        \"provider\": \"gemini\",\n",
    "        \"model\": \"gemini-1.5-flash\",\n",
    "        \"api_key\": \"GEMINI-API-KEY\",  # Replace with your Gemini API key\n",
    "        \"system_content\": \"You are an assistant that answers questions concisely.\"\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing gemini...\n",
      "Original Response: Paris\n",
      "\n",
      "Perturb Response: None\n",
      "Score Original: 1.0000001192092896\n",
      "Score Perturb: None\n"
     ]
    }
   ],
   "source": [
    "# Run tests for each provider\n",
    "for config in configs:\n",
    "    print(f\"\\nTesting {config['provider']}...\")\n",
    "\n",
    "    # Initialize the PromptCompletion instance\n",
    "    try:\n",
    "        completion = PromptCompletion(\n",
    "            model_provider=config[\"provider\"],\n",
    "            model=config[\"model\"],\n",
    "            system_content=config[\"system_content\"],\n",
    "            api_key=config[\"api_key\"],\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            max_tokens=100\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"Failed to initialize {config['provider']}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Create a test case\n",
    "    test = Test(\n",
    "        name=f\"{config['provider'].capitalize()} Test\",\n",
    "        prompt=test_prompt,\n",
    "        expected_result=expected_answer,\n",
    "        description=f\"A basic test for {config['provider']} integration.\"\n",
    "    )\n",
    "\n",
    "    # Run the test\n",
    "    try:\n",
    "        test.run(completion)\n",
    "        print(f\"Original Response: {test.original_response}\")\n",
    "        print(f\"Perturb Response: {test.perturb_response}\")\n",
    "        print(f\"Score Original: {test.score_original}\")\n",
    "        print(f\"Score Perturb: {test.score_perturb}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during test execution for {config['provider']}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing openai...\n",
      "Error during test execution for openai: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n",
      "\n",
      "Testing claude...\n",
      "Original Response:  The capital of France is Paris.\n",
      "Perturb Response: None\n",
      "Score Original: 0.5687420964241028\n",
      "Score Perturb: None\n",
      "\n",
      "Testing gemini...\n",
      "Error during test execution for gemini: bad argument type for built-in operation\n"
     ]
    }
   ],
   "source": [
    "# # Define configurations for each model provider\n",
    "# configs = [\n",
    "#     {\n",
    "#         \"provider\": \"openai\",\n",
    "#         \"model\": \"gpt-3.5-turbo\",\n",
    "#         \"api_key\": \"your_openai_api_key_here\",  # Replace with your OpenAI API key\n",
    "#         \"system_content\": \"You are an assistant that answers questions concisely.\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"provider\": \"claude\",\n",
    "#         \"model\": \"claude-2\",\n",
    "#         \"api_key\": \"your_claude_api_key_here\",  # Replace with your Claude API key\n",
    "#         \"system_content\": \"You are an assistant that answers questions concisely.\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"provider\": \"gemini\",\n",
    "#         \"model\": \"gemini-1.5-pro\",\n",
    "#         \"api_key\": \"your_gemini_api_key_here\",  # Replace with your Gemini API key\n",
    "#         \"system_content\": \"You are an assistant that answers questions concisely.\"\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# Run tests for each provider\n",
    "for config in configs:\n",
    "    print(f\"\\nTesting {config['provider']}...\")\n",
    "\n",
    "    # Initialize the PromptCompletion instance\n",
    "    try:\n",
    "        completion = PromptCompletion(\n",
    "            model_provider=config[\"provider\"],\n",
    "            model=config[\"model\"],\n",
    "            system_content=config[\"system_content\"],\n",
    "            api_key=config[\"api_key\"],\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            max_tokens=100\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"Failed to initialize {config['provider']}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Create a test case\n",
    "    test = Test(\n",
    "        name=f\"{config['provider'].capitalize()} Test\",\n",
    "        prompt=test_prompt,\n",
    "        expected_result=expected_answer,\n",
    "        description=f\"A basic test for {config['provider']} integration.\"\n",
    "    )\n",
    "\n",
    "    # Run the test\n",
    "    try:\n",
    "        test.run(completion)\n",
    "        print(f\"Original Response: {test.original_response}\")\n",
    "        print(f\"Perturb Response: {test.perturb_response}\")\n",
    "        print(f\"Score Original: {test.score_original}\")\n",
    "        print(f\"Score Perturb: {test.score_perturb}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during test execution for {config['provider']}: {e}\")\n",
    "# Open-API อาจานหมด\n",
    "# Gemini รันอันข้างบน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
